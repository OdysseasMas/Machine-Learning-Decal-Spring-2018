{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This homework is intended as a brief overview of the machine learning process and the various topics you will learn in this class. We hope that this exercise will allow you to put in context the information you learn with us this semester. Don't worry if you don't understand the techniques here (that's what you'll learn this semester!); we just want to show you how you can use sklearn to do simple machine learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let us import some libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LinearRegression, Ridge, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this homework assignment, we will be using the MNIST dataset. The MNIST data is a collection of black and white 28x28 images, each picturing a handwritten digit. These were collected from digits people write at the post office, and now this dataset is a standard benchmark to evaluate models against used in the machine learning community. This may take some time to download. If this errors out, try rerunning it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = fetch_mldata('MNIST original')\n",
    "X = mnist.data.astype('float64')\n",
    "y = mnist.target.astype('int64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first explore this data a little bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 784) (70000,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape, y.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The X matrix here contains all the digit pictures. The data is (n_samples x n_features), meaning this data contains 70,000 pictures, each with 784 features (the 28x28 image is flattened into a single row). The y vector contains the label for each digit, so we know which digit (or class - class means category) is in each picture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try and visualize this data a bit. Change around the index variable to explore more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x17e6ddd8>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12964c50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = 0 #15000, 28999, 67345\n",
    "image = X[index].reshape((28, 28))\n",
    "plt.title('Label is ' + str(y[index]))\n",
    "plt.imshow(image, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that each pixel value ranges from 0-255. When we train our models, a good practice is to *standardize* the data so different features can be compared more equally. Here we will use a simple standardization, squeezing all values into the 0-1 interval range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we train our model, we want it to have the lowest error. Error presents itself in 2 ways: bias (how close our model is to the ideal model), and variance (how much our model varies with different datasets). If we train our model on a chunk of data, and then test our model on that same data, we will only witness the first type of error - bias. However, if we test on new, unseen data, that will reflect both bias and variance. This is the reasoning behind cross validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we want to have 2 datasets, train and test, each used for the named purpose exclusively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will walk you through applying various models to try and achieve the lowest error rate on this data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of our labels is a number from 0-9. If we simply did regression on this data, the labels would imply some sort of ordering of the classes (ie the digit 8 is more of the digit 7 than the digit 3 is, etc. We can fix this issue by one-hot encoding our labels. So, instead of each label being a simple digit, each label is a vector of 10 entries. 9 of those entries are zero, and only 1 entry is equal to one, corresponding to the index of the digit. Let's take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000, 10)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = OneHotEncoder(sparse=False)\n",
    "y_hot = enc.fit_transform(y.reshape(-1, 1))\n",
    "y_train_hot = enc.transform(y_train.reshape(-1, 1))\n",
    "y_hot.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember how the first sample is the digit zero? Let's now look at the new label at that index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hot[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 3 steps to build your model: create the model, train the model, then use your model to make predictions). In the sklearn API, this is made very clear. First you instantiate the model (constructor), then you call train it with the `fit` method, then you can make predictions on new data with the `test` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's do a basic linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear = LinearRegression()\n",
    "linear.fit(X_train, y_train_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc:  0.8589714285714286\n",
      "test acc:  0.8516571428571429\n"
     ]
    }
   ],
   "source": [
    "# use trained model to predict both train and test sets\n",
    "y_train_pred = linear.predict(X_train)\n",
    "y_test_pred = linear.predict(X_test)\n",
    "\n",
    "# print accuracies\n",
    "print('train acc: ', accuracy_score(y_train_pred.argmax(axis=1), y_train))\n",
    "print('test acc: ', accuracy_score(y_test_pred.argmax(axis=1), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note on interpretability: you can view the weights of your model with `linear.coef_`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try and regularize by adding a penalty term to see if we can get anything better. We can penalize via the L2 norm, aka Ridge Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc:  0.8589333333333333\n",
      "test acc:  0.8518285714285714\n"
     ]
    }
   ],
   "source": [
    "ridge = Ridge(alpha=0.1)\n",
    "ridge.fit(X_train, y_train_hot)\n",
    "print('train acc: ', accuracy_score(ridge.predict(X_train).argmax(axis=1), y_train))\n",
    "print('test acc: ', accuracy_score(ridge.predict(X_test).argmax(axis=1), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The alpha controls how much to penalize the weights. Play around with it to see if you can improve the test accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you have seen how to use some basic models to fit and evaluate your data. You will now walk through working with more models. Fill in code where needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now do logistic regression. From now on, the models will automatically one-hot the labels (so we don't need to worry about it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc:  0.9179238095238095\n",
      "test acc:  0.9178285714285714\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression(C=0.01, multi_class='multinomial', solver='saga', tol=0.1)\n",
    "logreg.fit(X_train, y_train)\n",
    "print('train acc: ', accuracy_score(logreg.predict(X_train), y_train))\n",
    "print('test acc: ', accuracy_score(logreg.predict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our accuracy has jumped ~5%! Why is this? Logistic Regression is a more complex model - instead of computing raw scores as in linear regression, it does one extra step and squashes values between 0 and 1. This means our model now optimizes over *probabilities* instead of raw scores. This makes sense since our vectors are 1-hot encoded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The C hyperparameter controls inverse regularization strength (inverse for this model only). Reguralization is important to make sure our model doesn't overfit (perform much better on train data than test data). Play around with the C parameter to try and get better results! You should be able to hit 92%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Trees are a completely different type of classifier. They essentially break up the possible space by repeatedly \"splitting\" on features to keep narrowing down the possibilities. Decision Trees are normally individually very week, so we typically average them together in bunches called Random Forest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you have seen many examples for how to construct, fit, and evaluate a model. Now do the same for Random Forest using the [documentation here](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html). You should be able to create one easily without needing to specify any constructor parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test acc:  0.9698285714285714\n",
      "train acc:  0.9998857142857143\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(max_depth = 50, random_state= 0,n_estimators= 100,min_samples_split= 4)\n",
    "clf.fit(X_train, y_train)\n",
    "print('test acc: ', accuracy_score(clf.predict(X_test), y_test))\n",
    "print('train acc: ', accuracy_score(clf.predict(X_train), y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WOWZA! That train accuracy is amazing, let's see if we can boost up the test accuracy a bit (since that's what really counts). Try and play around with the hyperparameters to see if you can edge out more accuracy (look at the documentation for parameters in the constructor). Focus on `n_estimators`, `min_samples_split`, `max_features`. You should be able to hit ~97%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A support vector classifier is another completely different type of classifier. It tries to find the best separating hyperplane through your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SVC will toast our laptops unless we reduce the data dimensionality. Let's keep 80% of the variation, and get rid of the rest. (This will cause a slight drop in performance, but not by much)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=0.8, whiten=True)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now let's take a look at what that actually did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52500, 43)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_pca.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, before we had 784 (28x28) features! However, PCA found just 43 basis features that explain 80% of the data. So, we went to just 5% of the original input space, but we still retained 80% of the information! Nice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This [blog post](http://colah.github.io/posts/2014-10-Visualizing-MNIST/) explains dimensionality reduction with MNIST far better than I can. It's a short read (<10 mins), and it contains some pretty cool visualizations. Read it and jot down things you learned from the post or further questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc:  0.8915238095238095\n",
      "test acc:  0.8950857142857143\n"
     ]
    }
   ],
   "source": [
    "lsvc = LinearSVC(dual=False, tol=0.01)\n",
    "lsvc.fit(X_train_pca, y_train)\n",
    "print('train acc: ', accuracy_score(lsvc.predict(X_train_pca), y_train))\n",
    "print('test acc: ', accuracy_score(lsvc.predict(X_test_pca), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-It is hard to conceptualize higher dims.\n",
    "-By looking at 2 pix. at a time help us gain a little intuition but not much.\n",
    "-PCA => find the best horiz. + vert. angle\n",
    "-Defined the cost of the distance of 2 points in the original space and the visualization.(Just wondering here, this is fundamental sorry, but why is it bad for distances not to be the same? Is it because our approximations in comparison to the original space aren't good enough?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train our first SVC. The LinearSVC can only find a linear decision boundary (the hyperplane)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVMs are really interesting because they have something called the *dual formulation*, in which the computation is expressed as training point inner products. This means that data can be lifted into higher dimensions easily with this \"kernel trick\". Data that is not linearly separable in a lower dimension can be linearly separable in a higher dimension - which is why we conduct the transform. Let us experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A transformation that lifts the data into a higher-dimensional space is called a kernel. A polynomial kernel expands the feature space by computing all the polynomial cross terms to a specific degree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc:  0.9303809523809524\n",
      "test acc:  0.9277142857142857\n"
     ]
    }
   ],
   "source": [
    "psvc = SVC(kernel='poly', degree=1, tol=0.01, cache_size=4000)\n",
    "psvc.fit(X_train_pca, y_train)\n",
    "print('train acc: ', accuracy_score(psvc.predict(X_train_pca), y_train))\n",
    "print('test acc: ', accuracy_score(psvc.predict(X_test_pca), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Play around with the degree of the polynomial kernel to see what accuracy you can get."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RBF kernel uses the gaussian function to create an infinite dimensional space - a gaussian peak at each datapoint. Now fiddle with the `C` and `gamma` parameters of the gaussian kernel below to see what you can get. [Here's documentation](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc:  0.9996190476190476\n",
      "test acc:  0.9715428571428572\n"
     ]
    }
   ],
   "source": [
    "rsvc = SVC(kernel='rbf', tol=0.01, cache_size=4000,C = 1.0, gamma = 0.10000000000000001)\n",
    "rsvc.fit(X_train_pca, y_train)\n",
    "print('train acc: ', accuracy_score(rsvc.predict(X_train_pca), y_train))\n",
    "print('test acc: ', accuracy_score(rsvc.predict(X_test_pca), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isn't that just amazing accuracy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should never do neural networks in sklearn. Use Keras (which we will teach you later in this class), Tensorflow, PyTorch, etc. However, in an effort to keep this homework somewhat cohesive, let us proceed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic neural networks proceed in layers. Each layer has a certain number of nodes, representing how expressive that layer can be. Below is a sample network, with an input layer, one hidden (middle) layer of 50 neurons, and finally the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.01254076\n",
      "Iteration 2, loss = 0.32751621\n",
      "Iteration 3, loss = 0.23915568\n",
      "Iteration 4, loss = 0.19751170\n",
      "Iteration 5, loss = 0.17059502\n",
      "Iteration 6, loss = 0.15126529\n",
      "Iteration 7, loss = 0.13621792\n",
      "Iteration 8, loss = 0.12447136\n",
      "Iteration 9, loss = 0.11494617\n",
      "Iteration 10, loss = 0.10679176\n",
      "Iteration 11, loss = 0.09981043\n",
      "Iteration 12, loss = 0.09381982\n",
      "Iteration 13, loss = 0.08841084\n",
      "Iteration 14, loss = 0.08364485\n",
      "Iteration 15, loss = 0.07960517\n",
      "Iteration 16, loss = 0.07604351\n",
      "Iteration 17, loss = 0.07254231\n",
      "Iteration 18, loss = 0.06956803\n",
      "Iteration 19, loss = 0.06671120\n",
      "Iteration 20, loss = 0.06417710\n",
      "Iteration 21, loss = 0.06187784\n",
      "Iteration 22, loss = 0.05974931\n",
      "Iteration 23, loss = 0.05739583\n",
      "Iteration 24, loss = 0.05585498\n",
      "Iteration 25, loss = 0.05407596\n",
      "Iteration 26, loss = 0.05234284\n",
      "Iteration 27, loss = 0.05064165\n",
      "Iteration 28, loss = 0.04918727\n",
      "Iteration 29, loss = 0.04770698\n",
      "Iteration 30, loss = 0.04664624\n",
      "Iteration 31, loss = 0.04518638\n",
      "Iteration 32, loss = 0.04403251\n",
      "Iteration 33, loss = 0.04300997\n",
      "Iteration 34, loss = 0.04181048\n",
      "Iteration 35, loss = 0.04076342\n",
      "Iteration 36, loss = 0.03975030\n",
      "Iteration 37, loss = 0.03877849\n",
      "Iteration 38, loss = 0.03797901\n",
      "Iteration 39, loss = 0.03713841\n",
      "Iteration 40, loss = 0.03631333\n",
      "Iteration 41, loss = 0.03525449\n",
      "Iteration 42, loss = 0.03466339\n",
      "Iteration 43, loss = 0.03386499\n",
      "Iteration 44, loss = 0.03309855\n",
      "Iteration 45, loss = 0.03240580\n",
      "Iteration 46, loss = 0.03165026\n",
      "Iteration 47, loss = 0.03125218\n",
      "Iteration 48, loss = 0.03040673\n",
      "Iteration 49, loss = 0.02984574\n",
      "Iteration 50, loss = 0.02893843\n",
      "Iteration 51, loss = 0.02856855\n",
      "Iteration 52, loss = 0.02812534\n",
      "Iteration 53, loss = 0.02752400\n",
      "Iteration 54, loss = 0.02698096\n",
      "Iteration 55, loss = 0.02637162\n",
      "Iteration 56, loss = 0.02626186\n",
      "Iteration 57, loss = 0.02524655\n",
      "Iteration 58, loss = 0.02489274\n",
      "Iteration 59, loss = 0.02447820\n",
      "Iteration 60, loss = 0.02393401\n",
      "Iteration 61, loss = 0.02351949\n",
      "Iteration 62, loss = 0.02311629\n",
      "Iteration 63, loss = 0.02250454\n",
      "Iteration 64, loss = 0.02213151\n",
      "Iteration 65, loss = 0.02179007\n",
      "Iteration 66, loss = 0.02119444\n",
      "Iteration 67, loss = 0.02076577\n",
      "Iteration 68, loss = 0.02023529\n",
      "Iteration 69, loss = 0.02009694\n",
      "Iteration 70, loss = 0.01967978\n",
      "Iteration 71, loss = 0.01920875\n",
      "Iteration 72, loss = 0.01876024\n",
      "Iteration 73, loss = 0.01838159\n",
      "Iteration 74, loss = 0.01817495\n",
      "Iteration 75, loss = 0.01780424\n",
      "Iteration 76, loss = 0.01754374\n",
      "Iteration 77, loss = 0.01728434\n",
      "Iteration 78, loss = 0.01709405\n",
      "Iteration 79, loss = 0.01661481\n",
      "Iteration 80, loss = 0.01644136\n",
      "Iteration 81, loss = 0.01613165\n",
      "Iteration 82, loss = 0.01574348\n",
      "Iteration 83, loss = 0.01548615\n",
      "Iteration 84, loss = 0.01510562\n",
      "Iteration 85, loss = 0.01469885\n",
      "Iteration 86, loss = 0.01485058\n",
      "Iteration 87, loss = 0.01447543\n",
      "Iteration 88, loss = 0.01408642\n",
      "Iteration 89, loss = 0.01389315\n",
      "Iteration 90, loss = 0.01376749\n",
      "Iteration 91, loss = 0.01365156\n",
      "Iteration 92, loss = 0.01322730\n",
      "Iteration 93, loss = 0.01305004\n",
      "Iteration 94, loss = 0.01285659\n",
      "Iteration 95, loss = 0.01255851\n",
      "Iteration 96, loss = 0.01227519\n",
      "Iteration 97, loss = 0.01204527\n",
      "Iteration 98, loss = 0.01175572\n",
      "Iteration 99, loss = 0.01182303\n",
      "Iteration 100, loss = 0.01158196\n",
      "Iteration 101, loss = 0.01127795\n",
      "Iteration 102, loss = 0.01111277\n",
      "Iteration 103, loss = 0.01076580\n",
      "Iteration 104, loss = 0.01078729\n",
      "Iteration 105, loss = 0.01056234\n",
      "Iteration 106, loss = 0.01047111\n",
      "Iteration 107, loss = 0.01022370\n",
      "Iteration 108, loss = 0.01005072\n",
      "Iteration 109, loss = 0.00984248\n",
      "Iteration 110, loss = 0.00974749\n",
      "Iteration 111, loss = 0.00939729\n",
      "Iteration 112, loss = 0.00933676\n",
      "Iteration 113, loss = 0.00912403\n",
      "Iteration 114, loss = 0.00893841\n",
      "Iteration 115, loss = 0.00883802\n",
      "Iteration 116, loss = 0.00899741\n",
      "Iteration 117, loss = 0.00869182\n",
      "Iteration 118, loss = 0.00831723\n",
      "Iteration 119, loss = 0.00816780\n",
      "Iteration 120, loss = 0.00797559\n",
      "Iteration 121, loss = 0.00793632\n",
      "Iteration 122, loss = 0.00777010\n",
      "Iteration 123, loss = 0.00772077\n",
      "Iteration 124, loss = 0.00781756\n",
      "Iteration 125, loss = 0.00741389\n",
      "Iteration 126, loss = 0.00730178\n",
      "Iteration 127, loss = 0.00716668\n",
      "Iteration 128, loss = 0.00704242\n",
      "Iteration 129, loss = 0.00707805\n",
      "Iteration 130, loss = 0.00679371\n",
      "Iteration 131, loss = 0.00670616\n",
      "Iteration 132, loss = 0.00671115\n",
      "Iteration 133, loss = 0.00641973\n",
      "Iteration 134, loss = 0.00638538\n",
      "Iteration 135, loss = 0.00622427\n",
      "Iteration 136, loss = 0.00616862\n",
      "Iteration 137, loss = 0.00593977\n",
      "Iteration 138, loss = 0.00589148\n",
      "Iteration 139, loss = 0.00594575\n",
      "Iteration 140, loss = 0.00561810\n",
      "Iteration 141, loss = 0.00558298\n",
      "Iteration 142, loss = 0.00571186\n",
      "Iteration 143, loss = 0.00552673\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "train acc:  0.9996761904761905\n",
      "test acc:  0.9723428571428572\n"
     ]
    }
   ],
   "source": [
    "nn = MLPClassifier(hidden_layer_sizes=(79,), solver='adam', verbose=1)\n",
    "nn.fit(X_train_pca, y_train)\n",
    "print('train acc: ', accuracy_score(nn.predict(X_train_pca), y_train))\n",
    "print('test acc: ', accuracy_score(nn.predict(X_test_pca), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fiddle around with the hiddle layers. Change the number of neurons, add more layers, experiment. You should be able to hit 98% accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks are optimized with a technique called gradient descent (a neural net is just one big function - so we can take the gradient with respect to all its parameters, then just go opposite the gradient to try and find the minimum). This is why it requires many iterations to converge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turning In"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert this notebook to a PDF (file -> download as -> pdf via latex) and submit to Gradescope."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
